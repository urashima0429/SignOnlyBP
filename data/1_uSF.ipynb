{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f422c9f3940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cores: 20\n",
      "Available GPUs: ['NVIDIA GeForce RTX 4070 Laptop GPU']\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Available cores:\", os.cpu_count())\n",
    "\n",
    "# Check and display the GPU environment details using TensorFlow\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available GPUs:\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])\n",
    "    print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST\n",
      "Number of Channels: 1\n",
      "Width: 28\n",
      "Height: 28\n",
      "Number of Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Settings\n",
    "dataset = \"MNIST\"\n",
    "# dataset = \"FashionMNIST\"\n",
    "# dataset = \"CIFAR10\"\n",
    "\n",
    "if dataset == \"MNIST\":\n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "elif dataset == \"FashionMNIST\":\n",
    "    # Load Fashion-MNIST\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)), \n",
    "    ])\n",
    "    train_dataset = datasets.FashionMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.FashionMNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "elif dataset == \"CIFAR10\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])    \n",
    "    train_dataset = datasets.CIFAR10(root='../data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='../data', train=False, transform=transform, download=True)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
    "\n",
    "_smp_data, _ = train_dataset[0]\n",
    "num_channels, height, width = _smp_data.shape\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "print(\"Dataset:\", dataset)\n",
    "print(\"Number of Channels:\", num_channels)\n",
    "print(\"Width:\", width)\n",
    "print(\"Height:\", height)\n",
    "print(\"Number of Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class DefaultLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(DefaultLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class USFGradFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            # grad_input = grad_output.mm(weight) \n",
    "            grad_input = grad_output.mm( weight.sign() ) / math.sqrt(weight.size(1)) \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "            # grad_weight = grad_output.t().mm(input.sign()) / math.sqrt(input.size(1))\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "class USFLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(USFLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return USFGradFunction.apply(input, self.weight, self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SBPGradFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            # grad_input = grad_output.mm(weight) \n",
    "            grad_input = grad_output.mm( weight.sign() ) / math.sqrt(weight.size(1)) \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            # grad_weight = grad_output.t().mm(input)\n",
    "            grad_weight = grad_output.t().mm(input.sign()) / math.sqrt(input.size(1))\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "class SBPLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(SBPLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return SBPGradFunction.apply(input, self.weight, self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with DefaultLinear\n",
      "Epoch 1: Test set: Average loss: 0.1188, Accuracy: 96.28%\n",
      "Epoch 2: Test set: Average loss: 0.0912, Accuracy: 97.02%\n",
      "Epoch 3: Test set: Average loss: 0.0768, Accuracy: 97.50%\n",
      "Training with USFLinear\n",
      "Epoch 1: Test set: Average loss: 0.1541, Accuracy: 95.09%\n",
      "Epoch 2: Test set: Average loss: 0.0993, Accuracy: 96.91%\n",
      "Epoch 3: Test set: Average loss: 0.0760, Accuracy: 97.48%\n",
      "Training with SBPLinear\n",
      "Epoch 1: Test set: Average loss: 0.5307, Accuracy: 87.57%\n",
      "Epoch 2: Test set: Average loss: 0.4068, Accuracy: 89.50%\n",
      "Epoch 3: Test set: Average loss: 0.3631, Accuracy: 90.13%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, linear_layer):\n",
    "        super(Network, self).__init__()\n",
    "        self.layer1 = linear_layer(num_channels * height * width, 512)\n",
    "        self.layer2 = linear_layer(512, 256)\n",
    "        self.layer3 = linear_layer(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, device=None):\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}: Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Training with DefaultLinear\")\n",
    "default_model = Network(DefaultLinear)\n",
    "train_and_evaluate(default_model, train_loader, test_loader, device)\n",
    "\n",
    "print(\"Training with USFLinear\")\n",
    "usf_model = Network(USFLinear)\n",
    "train_and_evaluate(usf_model, train_loader, test_loader, device)\n",
    "\n",
    "print(\"Training with SBPLinear\")\n",
    "sbp_model = Network(SBPLinear)\n",
    "train_and_evaluate(sbp_model, train_loader, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
