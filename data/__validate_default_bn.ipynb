{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f3cfe467520>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cores: 16\n",
      "Available GPUs: ['NVIDIA GeForce RTX 2060 with Max-Q Design']\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Available cores:\", os.cpu_count())\n",
    "\n",
    "# Check and display the GPU environment details using TensorFlow\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available GPUs:\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])\n",
    "    print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "class DefaultBatchNorm2dGrad(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, running_mean, running_var, training, momentum, eps):\n",
    "        N, C, H, W = input.shape\n",
    "        if training or running_mean is None:\n",
    "            mean = input.mean([0, 2, 3], keepdim=True)  # Shape: [1, C, 1, 1]\n",
    "            var = input.var([0, 2, 3], keepdim=True, unbiased=False)  # Shape: [1, C, 1, 1]\n",
    "            if running_mean is not None:\n",
    "                running_mean.mul_(1 - momentum).add_(mean.squeeze() * momentum)\n",
    "                running_var.mul_(1 - momentum).add_(var.squeeze() * momentum)\n",
    "        else:\n",
    "            mean = running_mean.view(1, C, 1, 1)\n",
    "            var = running_var.view(1, C, 1, 1)\n",
    "\n",
    "        ctx.save_for_backward(input, weight, bias, mean, var)\n",
    "        ctx.eps = eps\n",
    "\n",
    "        input_normalized = (input - mean) / torch.sqrt(var + eps)\n",
    "        output = weight.view(1, C, 1, 1) * input_normalized + bias.view(1, C, 1, 1)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias, mean, var = ctx.saved_tensors\n",
    "        eps = ctx.eps\n",
    "        input_normalized = (input - mean) / torch.sqrt(var + eps)\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = (grad_output * weight.view(1, -1, 1, 1)).div(torch.sqrt(var + eps))\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = (grad_output * input_normalized).sum(dim=[0, 2, 3])\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(dim=[0, 2, 3])\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None, None\n",
    "\n",
    "class DefaultBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super(DefaultBatchNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        if affine:\n",
    "            self.weight = nn.Parameter(torch.ones(num_features))\n",
    "            self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        if track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "        else:\n",
    "            self.register_buffer('running_mean', None)\n",
    "            self.register_buffer('running_var', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.track_running_stats:\n",
    "            self.running_mean.zero_()\n",
    "            self.running_var.fill_(1)\n",
    "        if self.affine:\n",
    "            nn.init.ones_(self.weight)\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return DefaultBatchNorm2dGrad.apply(\n",
    "            input, \n",
    "            self.weight, \n",
    "            self.bias, \n",
    "            self.running_mean, \n",
    "            self.running_var, \n",
    "            self.training, \n",
    "            self.momentum, \n",
    "            self.eps\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdvancedConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, height, width, num_classes,\n",
    "                 conv_module=nn.Conv2d,\n",
    "                 linear_module=nn.Linear,\n",
    "                 bn_module=nn.BatchNorm2d,\n",
    "                 bn_options={'affine': True, 'track_running_stats': True}):\n",
    "        super(AdvancedConvNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = conv_module(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = bn_module(64, **bn_options)\n",
    "        self.conv2 = conv_module(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = bn_module(128, **bn_options)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = conv_module(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = bn_module(256, **bn_options)\n",
    "        self.conv4 = conv_module(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = bn_module(256, **bn_options)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        fc_input_features = 256 * (height // 4) * (width // 4)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.fc1 = linear_module(fc_input_features, 1024)\n",
    "        self.fc2 = linear_module(1024, 512)\n",
    "        self.fc3 = linear_module(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 256 * (self.height // 4) * (self.width // 4))\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        x = F.relu(self.fc2(self.dropout(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of Channels: 3\n",
      "Width: 32\n",
      "Height: 32\n",
      "Number of Classes: 10\n",
      "Training with DefaultBatchNorm2d, affine=True, track_running_stats=True:\n",
      "Epoch 1: Test Loss: 0.0273, Accuracy: 0.3739\n",
      "Training with nn.BatchNorm2d, affine=True, track_running_stats=True:\n",
      "Epoch 1: Test Loss: 0.0216, Accuracy: 0.5238\n",
      "Training with DefaultBatchNorm2d, affine=True, track_running_stats=False:\n",
      "Epoch 1: Test Loss: 0.0255, Accuracy: 0.3944\n",
      "Training with nn.BatchNorm2d, affine=True, track_running_stats=False:\n",
      "Epoch 1: Test Loss: 0.0180, Accuracy: 0.5880\n",
      "Training with DefaultBatchNorm2d, affine=False, track_running_stats=True:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DefaultBatchNorm2d' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m set_seed()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with DefaultBatchNorm2d, affine=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maffine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, track_running_stats=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrack_running_stats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDefaultBatchNorm2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m set_seed()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with nn.BatchNorm2d, affine=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maffine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, track_running_stats=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrack_running_stats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(bn_module, affine, track_running_stats)\u001b[0m\n\u001b[1;32m     57\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 59\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m, in \u001b[0;36mAdvancedConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m, in \u001b[0;36mDefaultBatchNorm2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DefaultBatchNorm2dGrad\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28minput\u001b[39m, \n\u001b[0;32m---> 70\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, \n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean, \n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var, \n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum, \n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DefaultBatchNorm2d' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "_smp_data, _ = train_dataset[0]\n",
    "num_channels, height, width = _smp_data.shape\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "print(\"Number of Channels:\", num_channels)\n",
    "print(\"Width:\", width)\n",
    "print(\"Height:\", height)\n",
    "print(\"Number of Classes:\", num_classes)\n",
    "\n",
    "\n",
    "def train_and_evaluate(bn_module, affine=True, track_running_stats=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AdvancedConvNet(num_channels, height, width, num_classes,\n",
    "                            bn_module=bn_module,\n",
    "                            bn_options={'affine': affine, 'track_running_stats': track_running_stats}).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}: Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "for affine in [True, False]:\n",
    "    for track_running_stats in [True, False]:\n",
    "        set_seed()\n",
    "        print(f\"Training with DefaultBatchNorm2d, affine={affine}, track_running_stats={track_running_stats}:\")\n",
    "        train_and_evaluate(DefaultBatchNorm2d, affine, track_running_stats)\n",
    "\n",
    "        set_seed()\n",
    "        print(f\"Training with nn.BatchNorm2d, affine={affine}, track_running_stats={track_running_stats}:\")\n",
    "        train_and_evaluate(nn.BatchNorm2d, affine, track_running_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
